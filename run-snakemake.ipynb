{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snakemake tutorial\n",
    "\n",
    "In this tutorial, we will learn how to operate snakemake to create executable workflows.\n",
    "\n",
    "## Objectives\n",
    "1. Basic understanding how dependencies between files is used in snakemake\n",
    "2. Execute snakemake on the commandline\n",
    "3. Being able to understand why and how map-reduce parallelism is pertinent to handle large inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hello, snakemake!\n",
    "Snakemake executes workflows which consist of multiple rules. Each rule is a unit/step in the data analysis. You can think of a typical data analysis workflow:\n",
    "1. Preprocessing the dataset\n",
    "2. Data cleansing and transforms\n",
    "3. Analyze the data (compute metrics, training models)\n",
    "4. Evaluate the results (calculate statistics, cross-validation)\n",
    "5. Plot the results\n",
    "\n",
    "We will model data analysis pipeline which resembles such a data workflow.\n",
    "Let's execute our very first rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/local/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\thello\n",
      "\t1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:16 2020]\u001b[0m\n",
      "\u001b[32mrule hello:\n",
      "    jobid: 0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\thello\n",
      "\t1\u001b[0m\n",
      "hello world\n",
      "\u001b[32m[Tue Feb 18 16:11:16 2020]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m1 of 1 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161116.564327.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! This worked well! The rule outputted ```hello world```, such a classic thing to do.\n",
    "\n",
    "Next, let's look up which rules exist in the ```Snakefile```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall\u001b[0m\r\n",
      "\u001b[32mgenerate_data\u001b[0m\r\n",
      "\u001b[32mchunk_dataset\u001b[0m\r\n",
      "\u001b[32madd_country\u001b[0m\r\n",
      "\u001b[32mmerge_results\u001b[0m\r\n",
      "\u001b[32mplot_results\u001b[0m\r\n",
      "\u001b[32mhello\u001b[0m\r\n",
      "\u001b[32mclean\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! snakemake --list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean snakemake\n",
    "\n",
    "Intriguing, this list gives us a clue how our rules are ordered:\n",
    "1. ```generate_data```: we first generate data, \n",
    "2. ```chunk_dataset```: chunk the data in multiple pieces, \n",
    "3. ```add_country```: apply a transform by adding a country to each observation,\n",
    "4. ```merge_results```: we merge the results from step 3,\n",
    "5. ```plot_results```: finally we plot the results.\n",
    "\n",
    "We already know what the rule ```hello``` does. We will cover what and how the rule ```all``` operates after we learned how the rules above are lined up.\n",
    "\n",
    "Let's focus on a much simpler rule: ```clean``` wipes intermediate output (data, plots, CSVs, etc.) of previous snakemake runs from our current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/local/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tclean\n",
      "\t1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:17 2020]\u001b[0m\n",
      "\u001b[32mrule clean:\n",
      "    jobid: 0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tclean\n",
      "\t1\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:17 2020]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m1 of 1 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161117.316585.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smooth, a nice, clean directory.\n",
    "\n",
    "Wait, what is this ```.snakemake``` directory? ```.snakemake``` stores information about when and which rules were executed and for how long. We will use these logs to plot some statistics after we ran some more comprehensive workflows. To do so, we will use Python, R and shell in interoperable rules without any code to glue them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a dataset\n",
    "The rule ```generate_data``` makes use of a Python script ```generate-data.py``` which outputs ```my_dataset.csv``` in the folder ```data```.\n",
    "\n",
    "We can either \n",
    "1. call the rule ```generate_data``` by typing: \n",
    "```bash\n",
    "$ snakemake generate_data\n",
    "```\n",
    "\n",
    "2. or request to generate the output file ```my_dataset.csv``` by executing:\n",
    "```bash\n",
    "$ snakemake data/my_dataset.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/local/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tgenerate_data\n",
      "\t1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:17 2020]\u001b[0m\n",
      "\u001b[32mrule generate_data:\n",
      "    output: data/my_dataset.csv\n",
      "    jobid: 0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "Generating a new dataset...\n",
      "Added observation: (site=site_A1, species=genus sp.a, abundance=40)\n",
      "Added observation: (site=site_A1, species=genus sp.b, abundance=7)\n",
      "Added observation: (site=site_A1, species=genus sp.c, abundance=1)\n",
      "Added observation: (site=site_A1, species=genus sp.d, abundance=17)\n",
      "Added observation: (site=site_A1, species=genus sp.e, abundance=15)\n",
      "Added observation: (site=site_A1, species=genus sp.f, abundance=14)\n",
      "Added observation: (site=site_A1, species=genus sp.g, abundance=8)\n",
      "Added observation: (site=site_A1, species=genus sp.h, abundance=6)\n",
      "Added observation: (site=site_A1, species=genus sp.i, abundance=34)\n",
      "Added observation: (site=site_A1, species=genus sp.j, abundance=5)\n",
      "Added observation: (site=site_A1, species=genus sp.k, abundance=37)\n",
      "Added observation: (site=site_A2, species=genus sp.a, abundance=27)\n",
      "Added observation: (site=site_A2, species=genus sp.b, abundance=2)\n",
      "Added observation: (site=site_A2, species=genus sp.c, abundance=1)\n",
      "Added observation: (site=site_A2, species=genus sp.d, abundance=5)\n",
      "Added observation: (site=site_A2, species=genus sp.e, abundance=13)\n",
      "Added observation: (site=site_A2, species=genus sp.f, abundance=14)\n",
      "Added observation: (site=site_A2, species=genus sp.g, abundance=32)\n",
      "Added observation: (site=site_A2, species=genus sp.h, abundance=38)\n",
      "Added observation: (site=site_A2, species=genus sp.i, abundance=1)\n",
      "Added observation: (site=site_A2, species=genus sp.j, abundance=35)\n",
      "Added observation: (site=site_A2, species=genus sp.k, abundance=12)\n",
      "Added observation: (site=site_A3, species=genus sp.a, abundance=41)\n",
      "Added observation: (site=site_A3, species=genus sp.b, abundance=34)\n",
      "Added observation: (site=site_A3, species=genus sp.c, abundance=26)\n",
      "Added observation: (site=site_A3, species=genus sp.d, abundance=14)\n",
      "Added observation: (site=site_A3, species=genus sp.e, abundance=28)\n",
      "Added observation: (site=site_A3, species=genus sp.f, abundance=37)\n",
      "Added observation: (site=site_A3, species=genus sp.g, abundance=17)\n",
      "Added observation: (site=site_A3, species=genus sp.h, abundance=0)\n",
      "Added observation: (site=site_A3, species=genus sp.i, abundance=10)\n",
      "Added observation: (site=site_A3, species=genus sp.j, abundance=27)\n",
      "Added observation: (site=site_A3, species=genus sp.k, abundance=21)\n",
      "Added observation: (site=site_A4, species=genus sp.a, abundance=17)\n",
      "Added observation: (site=site_A4, species=genus sp.b, abundance=9)\n",
      "Added observation: (site=site_A4, species=genus sp.c, abundance=13)\n",
      "Added observation: (site=site_A4, species=genus sp.d, abundance=21)\n",
      "Added observation: (site=site_A4, species=genus sp.e, abundance=6)\n",
      "Added observation: (site=site_A4, species=genus sp.f, abundance=5)\n",
      "Added observation: (site=site_A4, species=genus sp.g, abundance=24)\n",
      "Added observation: (site=site_A4, species=genus sp.h, abundance=6)\n",
      "Added observation: (site=site_A4, species=genus sp.i, abundance=22)\n",
      "Added observation: (site=site_A4, species=genus sp.j, abundance=22)\n",
      "Added observation: (site=site_A4, species=genus sp.k, abundance=38)\n",
      "Completed saving dataset as CSV.\n",
      "\u001b[32m[Tue Feb 18 16:11:18 2020]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m1 of 1 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161117.814409.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake generate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat, we created a dataset from random samples.\n",
    "\n",
    "If we try to rerun the same rule, ```snakemake``` detects that we already generated ```my_dataset.csv```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mNothing to be done.\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161118.782227.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake generate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, nothing happens.\n",
    "\n",
    "### 4. Split the dataset\n",
    "\n",
    "Next, we chunk the dataset into multiple pieces of same size by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/local/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tchunk_dataset\n",
      "\t1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:19 2020]\u001b[0m\n",
      "\u001b[32mrule chunk_dataset:\n",
      "    input: data/my_dataset.csv\n",
      "    output: data/blocks/subset_0.csv, data/blocks/subset_1.csv, data/blocks/subset_2.csv, data/blocks/subset_3.csv\n",
      "    jobid: 0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:19 2020]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m1 of 1 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161119.103627.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake chunk_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have four (nearly) equal-sized chunk of our dataset. So, why? Let's start our excursion into parallelism:\n",
    "\n",
    "## 5. Map-reduce parallelism\n",
    "Map-reduce paradigms become popular to handle large amount of data in parallel (e.g., [Apache Hadoop](https://hadoop.apache.org/) or [Apache Spark](https://spark.apache.org/) among others). The main idea of a map-reduce approach is a two-phase computation:\n",
    "1. Map step: apply a function to each element/sublist and yield a transformed return value, e.g. x => 2**x\n",
    "2. Reduce step: merge the elements from 1. to a data collection back or fold them to discrete value\n",
    "\n",
    "The map step can be carried out in parallel, whereas the reduce step requires that all map steps have successfully been carried out beforehand.\n",
    "\n",
    "Snakemake support such a parallelization in workflows. Let's investigate which rules can be executed in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"/Users/mk21womu/miniconda3/envs/snakemake-tutorial/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake --dag merge_results | dot -Tsvg >dag.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dag.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ```add_country``` appends a country to each observation of the dataset. As this operation is independent from another observation, we are able to execute this rule in parallel as a map function.\n",
    "2. After we completed ```add_country``` for each of the four subsets, we merge the results into a single CSV.\n",
    "\n",
    "Alright, let's trigger the execution of the ```merge_results``` rule and it's preceding steps which hasn't been executed thus far (the ones with solid edges):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/local/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t4\tadd_country\n",
      "\t1\tmerge_results\n",
      "\t5\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mrule add_country:\n",
      "    input: data/blocks/subset_1.csv\n",
      "    output: results/blocks/datasubset_w_country_1.csv\n",
      "    jobid: 2\n",
      "    wildcards: chunk=1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mrule add_country:\n",
      "    input: data/blocks/subset_0.csv\n",
      "    output: results/blocks/datasubset_w_country_0.csv\n",
      "    jobid: 1\n",
      "    wildcards: chunk=0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mrule add_country:\n",
      "    input: data/blocks/subset_3.csv\n",
      "    output: results/blocks/datasubset_w_country_3.csv\n",
      "    jobid: 4\n",
      "    wildcards: chunk=3\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mrule add_country:\n",
      "    input: data/blocks/subset_2.csv\n",
      "    output: results/blocks/datasubset_w_country_2.csv\n",
      "    jobid: 3\n",
      "    wildcards: chunk=2\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "[1] \"Adding countries...\"\n",
      "[1] \"Done with file = results/blocks/datasubset_w_country_3.csv\"\n",
      "[1] \"Adding countries...\"\n",
      "[1] \"Done with file = results/blocks/datasubset_w_country_2.csv\"\n",
      "[1] \"Adding countries...\"\n",
      "[1] \"Done with file = results/blocks/datasubset_w_country_0.csv\"\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mFinished job 4.\u001b[0m\n",
      "\u001b[32m1 of 5 steps (20%) done\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mFinished job 3.\u001b[0m\n",
      "\u001b[32m2 of 5 steps (40%) done\u001b[0m\n",
      "[1] \"Adding countries...\"\n",
      "[1] \"Done with file = results/blocks/datasubset_w_country_1.csv\"\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mFinished job 1.\u001b[0m\n",
      "\u001b[32m3 of 5 steps (60%) done\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mFinished job 2.\u001b[0m\n",
      "\u001b[32m4 of 5 steps (80%) done\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mrule merge_results:\n",
      "    input: results/blocks/datasubset_w_country_0.csv, results/blocks/datasubset_w_country_1.csv, results/blocks/datasubset_w_country_2.csv, results/blocks/datasubset_w_country_3.csv\n",
      "    output: results/dataset_results.csv\n",
      "    jobid: 0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:20 2020]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m5 of 5 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161120.376772.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake merge_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the output of the computations are interleaving. This is due to the parallelization.\n",
    "\n",
    "## 6. Plot the results\n",
    "Finally, we plot the results by making use of ggplot. We bin the abundances and show often these abundances occur in specific countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/local/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tplot_results\n",
      "\t1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:21 2020]\u001b[0m\n",
      "\u001b[32mrule plot_results:\n",
      "    input: results/dataset_results.csv\n",
      "    output: plots/abundance_histogram.png\n",
      "    jobid: 0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n",
      "Saving 7 x 7 in image\n",
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n",
      "\u001b[32m[Tue Feb 18 16:11:22 2020]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m1 of 1 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161121.240098.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake plot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plots/abundance_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Snakemake reports\n",
    "Snakemake comes with batteries loaded to provide run-time statistics.\n",
    "1. We wipe our output directories by running \n",
    "```bash\n",
    "$ snakemake clean\n",
    "```\n",
    "2. We request to run the entire pipeline:\n",
    "```bash\n",
    "$ snakemake\n",
    "```\n",
    "3. Finally we generate a report as a html webpage:\n",
    "```bash\n",
    "$ snakemake --report report.html\n",
    "```\n",
    "4. We inspect the report with a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/local/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tclean\n",
      "\t1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:23 2020]\u001b[0m\n",
      "\u001b[32mrule clean:\n",
      "    jobid: 0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tclean\n",
      "\t1\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:23 2020]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m1 of 1 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161123.150446.snakemake.log\u001b[0m\n",
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/local/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 4\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t4\tadd_country\n",
      "\t1\tall\n",
      "\t1\tchunk_dataset\n",
      "\t1\tgenerate_data\n",
      "\t1\tmerge_results\n",
      "\t1\tplot_results\n",
      "\t9\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:23 2020]\u001b[0m\n",
      "\u001b[32mrule generate_data:\n",
      "    output: data/my_dataset.csv\n",
      "    jobid: 8\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "Generating a new dataset...\n",
      "Added observation: (site=site_A1, species=genus sp.a, abundance=40)\n",
      "Added observation: (site=site_A1, species=genus sp.b, abundance=7)\n",
      "Added observation: (site=site_A1, species=genus sp.c, abundance=1)\n",
      "Added observation: (site=site_A1, species=genus sp.d, abundance=17)\n",
      "Added observation: (site=site_A1, species=genus sp.e, abundance=15)\n",
      "Added observation: (site=site_A1, species=genus sp.f, abundance=14)\n",
      "Added observation: (site=site_A1, species=genus sp.g, abundance=8)\n",
      "Added observation: (site=site_A1, species=genus sp.h, abundance=6)\n",
      "Added observation: (site=site_A1, species=genus sp.i, abundance=34)\n",
      "Added observation: (site=site_A1, species=genus sp.j, abundance=5)\n",
      "Added observation: (site=site_A1, species=genus sp.k, abundance=37)\n",
      "Added observation: (site=site_A2, species=genus sp.a, abundance=27)\n",
      "Added observation: (site=site_A2, species=genus sp.b, abundance=2)\n",
      "Added observation: (site=site_A2, species=genus sp.c, abundance=1)\n",
      "Added observation: (site=site_A2, species=genus sp.d, abundance=5)\n",
      "Added observation: (site=site_A2, species=genus sp.e, abundance=13)\n",
      "Added observation: (site=site_A2, species=genus sp.f, abundance=14)\n",
      "Added observation: (site=site_A2, species=genus sp.g, abundance=32)\n",
      "Added observation: (site=site_A2, species=genus sp.h, abundance=38)\n",
      "Added observation: (site=site_A2, species=genus sp.i, abundance=1)\n",
      "Added observation: (site=site_A2, species=genus sp.j, abundance=35)\n",
      "Added observation: (site=site_A2, species=genus sp.k, abundance=12)\n",
      "Added observation: (site=site_A3, species=genus sp.a, abundance=41)\n",
      "Added observation: (site=site_A3, species=genus sp.b, abundance=34)\n",
      "Added observation: (site=site_A3, species=genus sp.c, abundance=26)\n",
      "Added observation: (site=site_A3, species=genus sp.d, abundance=14)\n",
      "Added observation: (site=site_A3, species=genus sp.e, abundance=28)\n",
      "Added observation: (site=site_A3, species=genus sp.f, abundance=37)\n",
      "Added observation: (site=site_A3, species=genus sp.g, abundance=17)\n",
      "Added observation: (site=site_A3, species=genus sp.h, abundance=0)\n",
      "Added observation: (site=site_A3, species=genus sp.i, abundance=10)\n",
      "Added observation: (site=site_A3, species=genus sp.j, abundance=27)\n",
      "Added observation: (site=site_A3, species=genus sp.k, abundance=21)\n",
      "Added observation: (site=site_A4, species=genus sp.a, abundance=17)\n",
      "Added observation: (site=site_A4, species=genus sp.b, abundance=9)\n",
      "Added observation: (site=site_A4, species=genus sp.c, abundance=13)\n",
      "Added observation: (site=site_A4, species=genus sp.d, abundance=21)\n",
      "Added observation: (site=site_A4, species=genus sp.e, abundance=6)\n",
      "Added observation: (site=site_A4, species=genus sp.f, abundance=5)\n",
      "Added observation: (site=site_A4, species=genus sp.g, abundance=24)\n",
      "Added observation: (site=site_A4, species=genus sp.h, abundance=6)\n",
      "Added observation: (site=site_A4, species=genus sp.i, abundance=22)\n",
      "Added observation: (site=site_A4, species=genus sp.j, abundance=22)\n",
      "Added observation: (site=site_A4, species=genus sp.k, abundance=38)\n",
      "Completed saving dataset as CSV.\n",
      "\u001b[32m[Tue Feb 18 16:11:24 2020]\u001b[0m\n",
      "\u001b[32mFinished job 8.\u001b[0m\n",
      "\u001b[32m1 of 9 steps (11%) done\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:24 2020]\u001b[0m\n",
      "\u001b[32mrule chunk_dataset:\n",
      "    input: data/my_dataset.csv\n",
      "    output: data/blocks/subset_0.csv, data/blocks/subset_1.csv, data/blocks/subset_2.csv, data/blocks/subset_3.csv\n",
      "    jobid: 7\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:24 2020]\u001b[0m\n",
      "\u001b[32mFinished job 7.\u001b[0m\n",
      "\u001b[32m2 of 9 steps (22%) done\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:24 2020]\u001b[0m\n",
      "\u001b[32mrule add_country:\n",
      "    input: data/blocks/subset_1.csv\n",
      "    output: results/blocks/datasubset_w_country_1.csv\n",
      "    jobid: 4\n",
      "    wildcards: chunk=1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:24 2020]\u001b[0m\n",
      "\u001b[32mrule add_country:\n",
      "    input: data/blocks/subset_3.csv\n",
      "    output: results/blocks/datasubset_w_country_3.csv\n",
      "    jobid: 6\n",
      "    wildcards: chunk=3\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:24 2020]\u001b[0m\n",
      "\u001b[32mrule add_country:\n",
      "    input: data/blocks/subset_0.csv\n",
      "    output: results/blocks/datasubset_w_country_0.csv\n",
      "    jobid: 3\n",
      "    wildcards: chunk=0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:24 2020]\u001b[0m\n",
      "\u001b[32mrule add_country:\n",
      "    input: data/blocks/subset_2.csv\n",
      "    output: results/blocks/datasubset_w_country_2.csv\n",
      "    jobid: 5\n",
      "    wildcards: chunk=2\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "[1] \"Adding countries...\"\n",
      "[1] \"Done with file = results/blocks/datasubset_w_country_3.csv\"\n",
      "[1] \"Adding countries...\"\n",
      "[1] \"Done with file = results/blocks/datasubset_w_country_1.csv\"\n",
      "[1] \"Adding countries...\"\n",
      "[1] \"Done with file = results/blocks/datasubset_w_country_0.csv\"\n",
      "[1] \"Adding countries...\"\n",
      "\u001b[32m[Tue Feb 18 16:11:25 2020]\u001b[0m\n",
      "\u001b[32mFinished job 6.\u001b[0m\n",
      "[1]\u001b[32m3 of 9 steps (33%) done\u001b[0m\n",
      " \"Done with file = results/blocks/datasubset_w_country_2.csv\"\n",
      "\u001b[32m[Tue Feb 18 16:11:25 2020]\u001b[0m\n",
      "\u001b[32mFinished job 4.\u001b[0m\n",
      "\u001b[32m4 of 9 steps (44%) done\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:25 2020]\u001b[0m\n",
      "\u001b[32mFinished job 3.\u001b[0m\n",
      "\u001b[32m5 of 9 steps (56%) done\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:25 2020]\u001b[0m\n",
      "\u001b[32mFinished job 5.\u001b[0m\n",
      "\u001b[32m6 of 9 steps (67%) done\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:25 2020]\u001b[0m\n",
      "\u001b[32mrule merge_results:\n",
      "    input: results/blocks/datasubset_w_country_0.csv, results/blocks/datasubset_w_country_1.csv, results/blocks/datasubset_w_country_2.csv, results/blocks/datasubset_w_country_3.csv\n",
      "    output: results/dataset_results.csv\n",
      "    jobid: 2\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:25 2020]\u001b[0m\n",
      "\u001b[32mFinished job 2.\u001b[0m\n",
      "\u001b[32m7 of 9 steps (78%) done\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:25 2020]\u001b[0m\n",
      "\u001b[32mrule plot_results:\n",
      "    input: results/dataset_results.csv\n",
      "    output: plots/abundance_histogram.png\n",
      "    jobid: 1\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n",
      "Saving 7 x 7 in image\n",
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n",
      "\u001b[32m[Tue Feb 18 16:11:26 2020]\u001b[0m\n",
      "\u001b[32mFinished job 1.\u001b[0m\n",
      "\u001b[32m8 of 9 steps (89%) done\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Tue Feb 18 16:11:26 2020]\u001b[0m\n",
      "\u001b[32mrule all:\n",
      "    input: plots/abundance_histogram.png\n",
      "    jobid: 0\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tall\n",
      "\t1\u001b[0m\n",
      "Done with executing the workflow.\n",
      "The results are stored here: \n",
      "\u001b[32m[Tue Feb 18 16:11:26 2020]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m9 of 9 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: /Users/mk21womu/code/snakemake-tutorial/.snakemake/log/2020-02-18T161123.563650.snakemake.log\u001b[0m\n",
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mCreating report...\u001b[0m\n",
      "/Users/mk21womu/miniconda3/envs/snakemake-tutorial/lib/python3.7/site-packages/pygraphviz/agraph.py:1341: RuntimeWarning: Warning: Could not load \"/Users/mk21womu/miniconda3/envs/snakemake-tutorial/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n",
      "\u001b[33mLoading script code for rule plot_results\u001b[0m\n",
      "\u001b[33mLoading script code for rule add_country\u001b[0m\n",
      "\u001b[33mLoading script code for rule chunk_dataset\u001b[0m\n",
      "\u001b[33mLoading script code for rule generate_data\u001b[0m\n",
      "\u001b[33mReport created.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake clean && snakemake && snakemake --report report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum up\n",
    "We learned to integrate Python, bash and R to build reproducible workflows in Snakemake. A workflow consists of multiple rules. A rule in Snakemake defines its dependencies via input and output files. These dependencies permits us to model and run consecutive pipeline constituted of multiple rules. We covered more advanced features like parallelization of rules and report generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
